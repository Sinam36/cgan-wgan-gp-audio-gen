{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24_MNc_dGdMJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speechbrain"
      ],
      "metadata": {
        "id": "aquJg0mX9AD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kaJDbSJpaaE",
        "outputId": "0dca45bd-2fd4-40e9-e4b8-320d61535b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"drive/MyDrive/organized_dataset/\"\n",
        "TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
        "\n",
        "output_base = \"drive/MyDrive/FinalOutputs3/\"\n",
        "os.makedirs(os.path.join(output_base, \"gan_generated_audio\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_base, \"gan_spectrogram_plots\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_base, \"gan_comparison_plots\"), exist_ok=True)\n"
      ],
      "metadata": {
        "id": "OJtwXTU-GrlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 22050\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 256\n",
        "WIN_LENGTH = 1024\n",
        "N_MELS = 80\n",
        "MAX_FRAMES = 352"
      ],
      "metadata": {
        "id": "skYRyQwcGvjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LATENT_DIM = 100\n",
        "EPOCHS = 300\n",
        "BATCH_SIZE = 16\n",
        "LR_G = 1e-4\n",
        "LR_D = 2e-5\n",
        "BETAS = (0.0, 0.99)\n",
        "N_CRITIC = 3\n",
        "LAMBDA_GP = 4\n"
      ],
      "metadata": {
        "id": "d01EDvKVG1Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchaudio\")"
      ],
      "metadata": {
        "id": "Rx7wvXaSYFNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIFI_SRC = \"speechbrain/tts-hifigan-libritts-22050Hz\"\n",
        "HIFI_SAVE_DIR = \"pretrained_models/tts-hifigan-libritts-22050Hz\""
      ],
      "metadata": {
        "id": "nOjEmGIU9ZYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.inference.vocoders import HIFIGAN\n",
        "from speechbrain.lobes.models.FastSpeech2 import mel_spectogram\n"
      ],
      "metadata": {
        "id": "E3B6xSwR9eBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, root_dir, categories, max_frames=MAX_FRAMES):\n",
        "        self.root_dir = root_dir\n",
        "        self.categories = categories\n",
        "        self.max_frames = max_frames\n",
        "        self.file_list = []\n",
        "        self.class_to_idx = {cat: i for i, cat in enumerate(categories)}\n",
        "\n",
        "        for cat_name in categories:\n",
        "            cat_dir = os.path.join(root_dir, cat_name)\n",
        "            if not os.path.isdir(cat_dir): continue\n",
        "            wavs = [os.path.join(cat_dir, f) for f in os.listdir(cat_dir) if f.endswith(\".wav\")]\n",
        "            label_idx = self.class_to_idx[cat_name]\n",
        "            self.file_list.extend([(p, label_idx) for p in wavs])\n",
        "        self.sample_rate = SAMPLE_RATE\n",
        "        self.hop_length = HOP_LENGTH\n",
        "        self.win_length = WIN_LENGTH\n",
        "        self.n_fft = N_FFT\n",
        "        self.n_mels = N_MELS\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.file_list[idx]\n",
        "        wav, sr = torchaudio.load(path)\n",
        "        if wav.size(0) > 1: wav = wav.mean(dim=0, keepdim=True)\n",
        "        if sr != self.sample_rate:\n",
        "            wav = torchaudio.transforms.Resample(sr, self.sample_rate)(wav)\n",
        "        signal = wav.squeeze(0)\n",
        "\n",
        "        target_samples = (self.max_frames-1) * self.hop_length\n",
        "        current_samples = signal.shape[0]\n",
        "\n",
        "\n",
        "        if current_samples < target_samples:\n",
        "\n",
        "           signal = F.pad(signal, (0, target_samples - current_samples))\n",
        "        else:\n",
        "\n",
        "           signal = signal[:target_samples]\n",
        "        spectrogram, _ = mel_spectogram(\n",
        "            audio=signal,\n",
        "            sample_rate=self.sample_rate,\n",
        "            hop_length=self.hop_length,\n",
        "            win_length=self.win_length,\n",
        "            n_mels=self.n_mels,\n",
        "            n_fft=self.n_fft,\n",
        "            f_min=0.0,\n",
        "            f_max=8000.0,\n",
        "            power=1,\n",
        "            normalized=False,\n",
        "            min_max_energy_norm=False,\n",
        "            norm=\"slaney\",\n",
        "            mel_scale=\"slaney\",\n",
        "            compression=True\n",
        "        )\n",
        "        mel_input = spectrogram.unsqueeze(0)\n",
        "\n",
        "        label_vec = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()\n",
        "        return mel_input.float(), label_vec.float()\n"
      ],
      "metadata": {
        "id": "vr28KZIhHCGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_categories):\n",
        "        super().__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_categories = num_categories\n",
        "\n",
        "        self.unflatten_shape = (256, 5, 22)\n",
        "        flat_size = 256 * 5 * 22\n",
        "        self.fc = nn.Linear(latent_dim + num_categories, flat_size)\n",
        "\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 5->10, 22->44\n",
        "            nn.GroupNorm(8, 128),\n",
        "            nn.LeakyReLU(0.2,inplace = True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 10->20, 44->88\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.LeakyReLU(0.2,inplace = True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 20->40, 88->176\n",
        "            nn.GroupNorm(8, 32),\n",
        "            nn.LeakyReLU(0.2,inplace = True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),     # 40->80, 176->35\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "\n",
        "        h = torch.cat([z, y], dim=1)\n",
        "        h = self.fc(h)\n",
        "        h = h.view(-1, *self.unflatten_shape)\n",
        "        out = self.net(h)\n",
        "        out = torch.clamp(out, min=-15.0, max=1.0)          # [B, 1, 80, 352]\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "yCyS7DSdY8a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_categories, spec_shape=(80, 352)):\n",
        "        super().__init__()\n",
        "        H, W = spec_shape  # H=80, W=352\n",
        "\n",
        "        self.label_embedding = nn.Linear(num_categories, H * W)\n",
        "\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "\n",
        "            spectral_norm(nn.Conv2d(2, 32, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "\n",
        "            spectral_norm(nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "\n",
        "            spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.GroupNorm(8, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "\n",
        "            spectral_norm(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.GroupNorm(8, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "\n",
        "            spectral_norm(nn.Conv2d(256, 1, kernel_size=(5, 22), stride=1, padding=0)),\n",
        "        )\n",
        "\n",
        "    def forward(self, spec, y):\n",
        "\n",
        "        B = spec.size(0)\n",
        "\n",
        "        label_map = self.label_embedding(y).view(B, 1, *spec.shape[2:])  # [B,1,80,352]\n",
        "        label_map = label_map * 0.1  # keep conditional channel small vs spec energy\n",
        "\n",
        "        x = torch.cat([spec, label_map], dim=1)      # [B,2,80,352]\n",
        "        out = self.net(x)                            # [B,1,1,1]\n",
        "        return out.view(-1, 1)                       # [B,1]\n"
      ],
      "metadata": {
        "id": "fcqKV_zY_N3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
        "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "35vO9Q_mHNxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "hifi_gan = HIFIGAN.from_hparams(source=HIFI_SRC, savedir=HIFI_SAVE_DIR, run_opts={\"device\": DEVICE})\n",
        "hifi_gan.to(DEVICE).eval()\n"
      ],
      "metadata": {
        "id": "Aya8iEifFCvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generated_mel_to_waveform(mel_tensor, hifi_gan_model, device=DEVICE):\n",
        "\n",
        "    if mel_tensor.dim() == 4 and mel_tensor.shape[1] == 1:\n",
        "        mel = mel_tensor.squeeze(1)\n",
        "    elif  mel_tensor.dim() == 3 and mel_tensor.shape[0] == 1:\n",
        "        mel = mel_tensor\n",
        "    else: raise ValueError(f\"Unexpected mel shape: {mel_tensor.shape}. \")\n",
        "\n",
        "\n",
        "    mel = mel.to(device, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        wav = hifi_gan_model.decode_batch(mel)\n",
        "    if isinstance(wav, (tuple, list)):\n",
        "        wav = wav[0]\n",
        "\n",
        "\n",
        "    wav = wav.squeeze(1).detach().cpu()\n",
        "\n",
        "    if wav.dim() == 1:\n",
        "        wav = wav.unsqueeze(0)\n",
        "\n",
        "    return wav"
      ],
      "metadata": {
        "id": "YcXLQgOqbydD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_penalty(D, real_samples, fake_samples, labels, device, lambda_gp = LAMBDA_GP):\n",
        "    batch_size = real_samples.size(0)\n",
        "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device)\n",
        "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates, labels)\n",
        "    ones = torch.ones_like(d_interpolates, device=device)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=ones,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(batch_size, -1)\n",
        "    grad_norm = gradients.norm(2, dim=1)\n",
        "    gp = lambda_gp * ((grad_norm - 1) ** 2).mean()\n",
        "    return gp"
      ],
      "metadata": {
        "id": "yT1Pu4o42P1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, dataloader, device, categories, epochs, latent_dim ,output_base,vocoder,lr_g,lr_d ,real_samples_per_cat):\n",
        "\n",
        "    opt_G = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=BETAS)\n",
        "    opt_D = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=BETAS)\n",
        "\n",
        "\n",
        "    #  checkpoint paths\n",
        "    ckpt_path = os.path.join(output_base, \"checkpoint_last.pt\")\n",
        "\n",
        "\n",
        "    start_epoch = 1\n",
        "    if os.path.exists(ckpt_path):\n",
        "        ckpt = torch.load(ckpt_path, map_location=device)\n",
        "        generator.load_state_dict(ckpt['gen'])\n",
        "        discriminator.load_state_dict(ckpt['disc'])\n",
        "        opt_G.load_state_dict(ckpt['optG'])\n",
        "        opt_D.load_state_dict(ckpt['optD'])\n",
        "        start_epoch = ckpt.get('epoch', 1) + 1\n",
        "        print(\"Resumed from checkpoint epoch\", start_epoch-1)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs + 1):\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\")\n",
        "        for batch_i, (real_specs, labels) in enumerate(loop):\n",
        "            real_specs, labels = real_specs.to(device), labels.to(device)\n",
        "            B = real_specs.size(0)\n",
        "\n",
        "            # Train discriminator N_CRITIC times\n",
        "            for _ in range(N_CRITIC):\n",
        "                z = torch.randn(B, latent_dim, device=device)\n",
        "                fake_specs = generator(z, labels)\n",
        "\n",
        "                opt_D.zero_grad()\n",
        "                real_valid = discriminator(real_specs, labels)\n",
        "                fake_valid = discriminator(fake_specs.detach(), labels)\n",
        "\n",
        "                loss_D = -(real_valid.mean() - fake_valid.mean())\n",
        "                gp = compute_gradient_penalty(discriminator, real_specs, fake_specs.detach(), labels, device)\n",
        "                (loss_D + gp).backward()\n",
        "                opt_D.step()\n",
        "\n",
        "            # Train generator\n",
        "            opt_G.zero_grad()\n",
        "            z = torch.randn(B, latent_dim, device=device)\n",
        "            fake_specs = generator(z, labels)\n",
        "            fake_valid = discriminator(fake_specs, labels)\n",
        "            loss_G = -fake_valid.mean()\n",
        "\n",
        "            lambda_l1 = 1.0\n",
        "            l1_term = lambda_l1 * F.l1_loss(fake_specs, real_specs)\n",
        "            loss_G = loss_G + l1_term\n",
        "\n",
        "            loss_G.backward()\n",
        "            opt_G.step()\n",
        "            loop.set_postfix({\n",
        "            'loss_D': f\"{loss_D.item():.2f}\",\n",
        "             'loss_G': f\"{loss_G.item():.2f}\",\n",
        "             'GP': f\"{gp.item():.2f}\",\n",
        "            'L1': f\"{l1_term.item():.2f}\",\n",
        "             'D_real': f\"{real_valid.mean().item():.2f}\",\n",
        "              'D_fake': f\"{fake_valid.mean().item():.2f}\"\n",
        "              })\n",
        "\n",
        "        #  save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'gen': generator.state_dict(),\n",
        "            'disc': discriminator.state_dict(),\n",
        "            'optG': opt_G.state_dict(),\n",
        "            'optD': opt_D.state_dict()\n",
        "        }, ckpt_path)\n",
        "\n",
        "\n",
        "        if epoch%10 == 0:\n",
        "          generator.eval()\n",
        "          with torch.no_grad():\n",
        "\n",
        "            num_cats = len(categories)\n",
        "            torch.manual_seed(42)\n",
        "            z_fixed = torch.randn(num_cats, latent_dim, device=device)\n",
        "            y_fixed = F.one_hot(torch.arange(num_cats), num_classes=num_cats).float().to(device)\n",
        "            specs_gen = generator(z_fixed, y_fixed)\n",
        "\n",
        "            for i, cat in enumerate(categories):\n",
        "               spec_gen = specs_gen[i].squeeze().cpu()  # (80, 352)\n",
        "               plt.figure(figsize=(6,3))\n",
        "               plt.imshow(spec_gen.numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
        "               plt.title(f\"{cat} - epoch {epoch}\")\n",
        "               plt.axis('off')\n",
        "               plt.colorbar()\n",
        "               plt.tight_layout()\n",
        "               plt.savefig(os.path.join(output_base, \"gan_spectrogram_plots\", f\"{cat}_epoch{epoch}.png\"))\n",
        "               plt.close()\n",
        "\n",
        "\n",
        "\n",
        "               wav = generated_mel_to_waveform(specs_gen[i].unsqueeze(0), vocoder, device=device)  # (1,1,80,352) -> (1, time)\n",
        "               out_path = os.path.join(output_base, \"gan_generated_audio\", f\"{cat}_epoch{epoch}.wav\")\n",
        "               torchaudio.save(out_path, wav.unsqueeze(0) if wav.dim() == 1 else wav, SAMPLE_RATE)\n",
        "               print(\"[SAVED]\", out_path)\n",
        "\n",
        "\n",
        "\n",
        "               print(f\"\\n Category: {cat}, Epoch: {epoch}\")\n",
        "               print(\"spec_gen shape\", spec_gen.shape, \"min/max/mean/std:\", spec_gen.min().item(), spec_gen.max().item(), spec_gen.mean().item(), spec_gen.std().item())\n",
        "\n",
        "                   # Comparison plot\n",
        "            for i, cat in enumerate(categories):\n",
        "                    real_path = real_samples_per_cat.get(i)\n",
        "                    if real_path is not None:\n",
        "\n",
        "                        real_wav, sr = torchaudio.load(real_path)\n",
        "                        if real_wav.size(0) > 1: real_wav = real_wav.mean(dim=0, keepdim=True)\n",
        "                        if sr != SAMPLE_RATE: real_wav = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(real_wav)\n",
        "                        real_signal = real_wav.squeeze(0)\n",
        "\n",
        "\n",
        "                        target_samples = ( MAX_FRAMES - 1) * HOP_LENGTH\n",
        "                        if real_signal.shape[0] < target_samples:\n",
        "                            real_signal = F.pad(real_signal, (0, target_samples - real_signal.shape[0]))\n",
        "                        else:\n",
        "                            real_signal = real_signal[:target_samples]\n",
        "\n",
        "\n",
        "                        mel_real, _ = mel_spectogram(\n",
        "                            audio=real_signal,\n",
        "                            sample_rate=SAMPLE_RATE,\n",
        "                            hop_length=HOP_LENGTH,\n",
        "                            win_length=WIN_LENGTH,\n",
        "                            n_mels=N_MELS,\n",
        "                            n_fft=N_FFT,\n",
        "                            f_min=0.0, f_max=8000.0,\n",
        "                            power=1,\n",
        "                            normalized=False,\n",
        "                            min_max_energy_norm=False,\n",
        "                            norm=\"slaney\",\n",
        "                            mel_scale=\"slaney\",\n",
        "                            compression=True\n",
        "                        )\n",
        "                        logmel_real = mel_real.squeeze().numpy()\n",
        "\n",
        "                        spec_gen_np = specs_gen[i].squeeze().cpu().numpy()\n",
        "\n",
        "                        fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "                        im0 = ax[0].imshow(logmel_real, aspect='auto', origin='lower', cmap='viridis')\n",
        "                        ax[0].set_title(f\"Real {cat}\")\n",
        "                        ax[0].axis('off')\n",
        "                        plt.colorbar(im0, ax=ax[0])\n",
        "\n",
        "                        im1 = ax[1].imshow(spec_gen_np, aspect='auto', origin='lower', cmap='viridis')\n",
        "                        ax[1].set_title(f\"Gen {cat} (epoch {epoch})\")\n",
        "                        ax[1].axis('off')\n",
        "                        plt.colorbar(im1, ax=ax[1])\n",
        "\n",
        "                        plt.tight_layout()\n",
        "                        comp_path = os.path.join(output_base, \"gan_comparison_plots\", f\"{cat}_cmp_epoch{epoch}.png\")\n",
        "                        plt.savefig(comp_path)\n",
        "                        plt.close()\n",
        "\n",
        "            generator.train()\n"
      ],
      "metadata": {
        "id": "dyqfH69PHcIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    SEED = 42\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "    if not os.path.isdir(TRAIN_PATH):\n",
        "        raise RuntimeError(f\"TRAIN_PATH '{TRAIN_PATH}' not found.\")\n",
        "    train_categories = sorted([d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, d))])\n",
        "    print(f\"Found {len(train_categories)} categories: {train_categories}\")\n",
        "\n",
        "    dataset = AudioDataset(TRAIN_PATH, train_categories)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "    # Cache one real path per category for fast comparisons\n",
        "    real_samples_per_cat = {}\n",
        "    for path, label in dataset.file_list:\n",
        "     if label not in real_samples_per_cat:\n",
        "      real_samples_per_cat[label] = path\n",
        "\n",
        "\n",
        "\n",
        "    sample, _ = dataset[0]\n",
        "    print(\"Sample stats:\")\n",
        "    print(f\"min: {sample.min().item():.3f}, max: {sample.max().item():.3f}, mean: {sample.mean().item():.3f}, std: {sample.std().item():.3f}\")\n",
        "\n",
        "    gen = Generator(LATENT_DIM, len(train_categories)).to(DEVICE)\n",
        "    disc = Discriminator(len(train_categories)).to(DEVICE)\n",
        "    gen.apply(weights_init)\n",
        "    disc.apply(weights_init)\n",
        "\n",
        "    z = torch.randn(2, LATENT_DIM).to(DEVICE)\n",
        "    y = F.one_hot(torch.tensor([0, 1 % len(train_categories)]), num_classes=len(train_categories)).float().to(DEVICE)\n",
        "    fake = gen(z, y)\n",
        "    print(\"Generator output shape:\", fake.shape)\n",
        "    out_d = disc(fake, y)\n",
        "    print(\"Discriminator output shape:\", out_d.shape)\n",
        "\n",
        "    train_gan(gen, disc, dataloader, DEVICE, train_categories, EPOCHS, LATENT_DIM, output_base,vocoder=hifi_gan, lr_g = LR_G,lr_d = LR_D,real_samples_per_cat=real_samples_per_cat)\n"
      ],
      "metadata": {
        "id": "0793yX3EMLbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXMI1o-XtUKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}